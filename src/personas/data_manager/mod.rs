mod cache;
mod data;
mod mutable_data_cache;
mod mutation;
#[cfg(all(test, feature = "use-mock-routing"))]
mod tests;

use self::cache::{Cache, FragmentInfo, MutationVote, PendingWrite};
#[cfg(feature = "use-mock-crust")]
pub use self::cache::PENDING_WRITE_TIMEOUT_SECS;
pub use self::data::{Data, DataId, ImmutableDataId, MutableDataId};
use self::mutable_data_cache::MutableDataCache;
use self::mutation::{Mutation, MutationType};
use GROUP_SIZE;
use QUORUM;
use accumulator::Accumulator;
use authority::ClientManagerAuthority;
use chunk_store::{Chunk, ChunkId, ChunkStore};
#[cfg(feature = "use-mock-crust")]
use chunk_store::Error as ChunkStoreError;
use error::InternalError;
use maidsafe_utilities::serialisation;
use routing::{Authority, ClientError, EntryAction, ImmutableData, MessageId, MutableData,
              PermissionSet, RoutingTable, TYPE_TAG_SESSION_PACKET, User, Value, XorName};
use rust_sodium::crypto::sign;
use std::collections::{BTreeMap, BTreeSet};
use std::convert::From;
use std::fmt::{self, Debug, Formatter};
use std::path::PathBuf;
use std::time::Duration;
use tiny_keccak;
use utils::{self, HashMap, HashSet, Instant};
use vault::Refresh as VaultRefresh;
use vault::RoutingNode;

const MAX_FULL_PERCENT: u64 = 50;
/// The timeout for accumulating refresh messages.
const ACCUMULATOR_TIMEOUT_SECS: u64 = 180;
/// The interval for print status log.
const STATUS_LOG_INTERVAL: u64 = 120;

macro_rules! log_status {
    ($dm:expr) => {
        if $dm.logging_time.elapsed().as_secs() > STATUS_LOG_INTERVAL {
            $dm.logging_time = Instant::now();
            info!("{:?}", $dm);
        }
    }
}

impl Chunk<DataId> for ImmutableData {
    type Id = ImmutableDataId;
}

impl ChunkId<DataId> for ImmutableDataId {
    type Chunk = ImmutableData;
    fn to_key(&self) -> DataId {
        DataId::Immutable(*self)
    }
}

impl Chunk<DataId> for MutableData {
    type Id = MutableDataId;
}

impl ChunkId<DataId> for MutableDataId {
    type Chunk = MutableData;
    fn to_key(&self) -> DataId {
        DataId::Mutable(*self)
    }
}

pub struct DataManager {
    chunk_store: ChunkStore<DataId>,
    chunk_refresh_accumulator: Accumulator<MutableDataId, XorName>,
    fragment_refresh_accumulator: Accumulator<FragmentInfo, XorName>,
    cache: Cache,
    mdata_cache: MutableDataCache,
    immutable_data_count: u64,
    mutable_data_count: u64,
    client_get_requests: u64,
    logging_time: Instant,
    // This is only used in tests to indicate whether to delay the group refresh messages.
    _is_delaying_group_refresh: bool,
    // This is only used in tests as a place to temporarily hold incoming group refresh messages in
    // order to delay handling them.
    _delayed_group_refresh_cache: BTreeSet<Vec<u8>>,
}

impl DataManager {
    pub fn new(chunk_store_root: PathBuf, capacity: u64) -> Result<DataManager, InternalError> {
        let chunk_store = ChunkStore::new(chunk_store_root, capacity)?;
        let accumulator_duration = Duration::from_secs(ACCUMULATOR_TIMEOUT_SECS);

        Ok(DataManager {
               chunk_store: chunk_store,
               chunk_refresh_accumulator: Accumulator::with_duration(QUORUM, accumulator_duration),
               fragment_refresh_accumulator: Accumulator::with_duration(QUORUM,
                                                                        accumulator_duration),
               cache: Default::default(),
               mdata_cache: MutableDataCache::new(),
               immutable_data_count: 0,
               mutable_data_count: 0,
               client_get_requests: 0,
               logging_time: Instant::now(),
               // TODO: Once https://github.com/rust-lang/rust/issues/41681 is in stable we can
               // initialise the following two fields under #[cfg(feature = "use-mock-crust")] and
               // exclude the member variables from the struct for production builds altogether.
               _is_delaying_group_refresh: false,
               _delayed_group_refresh_cache: BTreeSet::new(),
           })
    }

    // When a new node joins a group, the other members of the group send it "refresh"
    // messages which contains info about the data chunks that this node needs to fetch
    // from them.
    //
    // Note: refresh is a message whose source is individual node, so its accumulation
    // must be performed manually.
    pub fn handle_serialised_refresh(&mut self,
                                     routing_node: &mut RoutingNode,
                                     src: XorName,
                                     serialised_refresh: &[u8])
                                     -> Result<(), InternalError> {
        let refreshes: Vec<Refresh> = serialisation::deserialise(serialised_refresh)?;
        self.handle_refreshes(routing_node, src, refreshes)
    }

    pub fn handle_refreshes(&mut self,
                            routing_node: &mut RoutingNode,
                            src: XorName,
                            refreshes: Vec<Refresh>)
                            -> Result<(), InternalError> {
        for refresh in refreshes {
            match refresh {
                Refresh::Chunk(data_id) => self.handle_chunk_refresh(src, data_id),
                Refresh::Fragment(fragment) => self.handle_fragment_refresh(src, fragment),
            }?
        }

        self.request_needed_data(routing_node)
    }

    // When a node in a group receives request to mutate some data it holds, it first sends
    // "group refresh" message to all the other members of the group. Only when the message
    // accumulates, the node applies the mutation to the data and updates it in the chunk
    // store.
    //
    // Note: group refresh is a message whose source is group, so the accumulation is
    // performed automatically by routing.
    pub fn handle_group_refresh(&mut self,
                                routing_node: &mut RoutingNode,
                                serialised_refresh: Vec<u8>)
                                -> Result<(), InternalError> {
        #[cfg(feature = "use-mock-crust")]
        // If this returns `None` then the message has been cached for handling later to simulate a
        // network delay.
        let serialised_refresh = match self.handle_delayed_group_refresh(serialised_refresh) {
            Some(serialised_refresh) => serialised_refresh,
            None => return Ok(()),
        };

        let MutationVote { data_id, hash } = serialisation::deserialise(&serialised_refresh)?;
        let write = match self.cache.take_pending_write(&data_id, &hash) {
            Some(write) => write,
            None => return Ok(()),
        };

        let PendingWrite {
            mutation,
            src,
            dst,
            message_id,
            ..
        } = write;

        let mutation_type = mutation.mutation_type();
        let fragments = self.commit_pending_mutation(routing_node, src, dst, mutation, message_id)?;

        if fragments.is_empty() {
            // The commit wasn't successful.
            return Ok(());
        }

        match mutation_type {
            MutationType::PutIData => self.immutable_data_count += 1,
            MutationType::PutMData => self.mutable_data_count += 1,
            _ => (),
        }

        log_status!(self);

        let refreshes = fragments.into_iter().map(Refresh::Fragment).collect();
        self.send_refresh(routing_node,
                          Authority::NaeManager(*data_id.name()),
                          refreshes)?;

        if let Some(group) = routing_node.close_group(*data_id.name(), GROUP_SIZE) {
            for node in &group {
                self.cache
                    .register_needed_data_with_another_holder(&data_id, *node);
            }

            self.request_needed_data(routing_node)?;
        }

        Ok(())
    }

    pub fn handle_get_idata(&mut self,
                            routing_node: &mut RoutingNode,
                            src: Authority<XorName>,
                            dst: Authority<XorName>,
                            name: XorName,
                            msg_id: MessageId)
                            -> Result<(), InternalError> {
        self.update_request_stats(&src);

        if let Ok(data) = self.chunk_store.get(&ImmutableDataId(name)) {
            trace!("As {:?} sending data {:?} to {:?}", dst, data, src);
            routing_node
                .send_get_idata_response(dst, src, Ok(data), msg_id)?;
        } else {
            trace!("DM sending get_idata_failure of {:?}", name);
            routing_node
                .send_get_idata_response(dst, src, Err(ClientError::NoSuchData), msg_id)?;
        }

        Ok(())
    }

    pub fn handle_get_idata_success(&mut self,
                                    routing_node: &mut RoutingNode,
                                    src: XorName,
                                    data: ImmutableData)
                                    -> Result<(), InternalError> {
        let mut valid = false;
        if let Some(fragment) = self.cache.stop_needed_fragment_request(&src) {
            if let FragmentInfo::ImmutableData(ref name) = fragment {
                if *name == *data.name() && *name == recompute_idata_name(&data) {
                    self.cache.remove_needed_fragment(&fragment);
                    valid = true;
                }
            }
        };

        self.request_needed_data(routing_node)?;

        if !valid {
            return Ok(());
        }

        // If we're no longer in the close group, return.
        if !close_to_address(routing_node, data.name()) {
            return Ok(());
        }

        let data_id = data.id();

        if self.chunk_store.has(&data_id) {
            return Ok(()); // data is already there.
        }

        self.clean_chunk_store();
        self.chunk_store.put(&data_id, &data)?;

        self.immutable_data_count += 1;
        log_status!(self);

        Ok(())
    }

    pub fn handle_get_idata_failure(&mut self,
                                    routing_node: &mut RoutingNode,
                                    src: XorName)
                                    -> Result<(), InternalError> {
        if self.cache.stop_needed_fragment_request(&src).is_none() {
            return Err(InternalError::InvalidMessage);
        }

        self.request_needed_data(routing_node)
    }

    pub fn handle_put_idata(&mut self,
                            routing_node: &mut RoutingNode,
                            src: Authority<XorName>,
                            dst: Authority<XorName>,
                            data: ImmutableData,
                            msg_id: MessageId)
                            -> Result<(), InternalError> {
        if self.chunk_store.has(&data.id()) {
            trace!("DM sending PutIData success for data {:?}, it already exists.",
                   data.name());
            routing_node
                .send_put_idata_response(dst, src, Ok(()), msg_id)?;
            return Ok(());
        }

        self.clean_chunk_store();

        if self.chunk_store_full() {
            let err = ClientError::NetworkFull;
            routing_node
                .send_put_idata_response(dst, src, Err(err.clone()), msg_id)?;
            return Err(From::from(err));
        }

        self.update_pending_writes(routing_node,
                                   Mutation::PutIData(data),
                                   src,
                                   dst,
                                   msg_id,
                                   false)
    }

    pub fn handle_get_mdata(&mut self,
                            routing_node: &mut RoutingNode,
                            src: Authority<XorName>,
                            name: XorName,
                            tag: u64,
                            msg_id: MessageId)
                            -> Result<(), InternalError> {
        self.update_request_stats(&src);

        /// Send the response as a node, to allow custom accumulation.
        let resp_src = Authority::ManagedNode(*routing_node.id()?.name());
        let resp_dst = src;

        let res = self.fetch_mdata(name, tag);
        trace!("As {:?} sending GetMData response {:?} of {:?} to {:?}",
               resp_src,
               res,
               name,
               resp_dst);
        routing_node
            .send_get_mdata_response(resp_src, resp_dst, res, msg_id)?;

        Ok(())
    }

    pub fn handle_get_mdata_success(&mut self,
                                    routing_node: &mut RoutingNode,
                                    src: XorName,
                                    data: MutableData,
                                    msg_id: MessageId)
                                    -> Result<(), InternalError> {
        self.cache
            .handle_needed_mutable_chunk_success(data.id(), src, msg_id);
        self.request_needed_data(routing_node)?;

        // If we're no longer in the close group, return.
        if !close_to_address(routing_node, data.name()) {
            return Ok(());
        }

        let data_id = data.id();
        let (shell, entries) = self.mdata_cache.accumulate(data, src);

        let (data, new) = match (shell, self.chunk_store.get(&data_id)) {
            (Some(mut shell), Ok(mut data)) => {
                if shell.version() > data.version() {
                    merge_mdata_entries(&mut shell, data.take_entries());
                    (Some(shell), false)
                } else {
                    (Some(data), false)
                }
            }
            (Some(mut shell), Err(_)) => {
                let entries = self.mdata_cache.take_entries(&data_id);
                merge_mdata_entries(&mut shell, entries);
                (Some(shell), true)
            }
            (None, Ok(data)) => (Some(data), false),
            (None, Err(_)) => (None, false),
        };

        if let Some(mut data) = data {
            merge_mdata_entries(&mut data, entries);
            self.clean_chunk_store();
            self.chunk_store.put(&data.id(), &data)?;
        } else {
            self.mdata_cache.insert_entries(data_id, entries);
        }

        if new {
            self.mutable_data_count += 1;
            log_status!(self);
        }

        Ok(())
    }

    pub fn handle_get_mdata_failure(&mut self,
                                    routing_node: &mut RoutingNode,
                                    src: XorName,
                                    msg_id: MessageId)
                                    -> Result<(), InternalError> {
        self.cache
            .handle_needed_mutable_chunk_failure(src, msg_id);
        self.request_needed_data(routing_node)
    }

    pub fn handle_put_mdata(&mut self,
                            routing_node: &mut RoutingNode,
                            src: Authority<XorName>,
                            dst: Authority<XorName>,
                            data: MutableData,
                            msg_id: MessageId,
                            _requester: sign::PublicKey)
                            -> Result<(), InternalError> {
        let data_id = data.id();

        let res = if self.chunk_store.has(&data_id) {
            Err(ClientError::DataExists)
        } else {
            self.clean_chunk_store();

            if self.chunk_store_full() {
                Err(ClientError::NetworkFull)
            } else {
                Ok(())
            }
        };

        let mutation = Mutation::PutMData(data);
        let res = res.and_then(|_| self.validate_concurrent_mutations(None, &mutation));
        let rejected = if let Err(error) = res {
            trace!("DM sending PutMData failure for data {:?}: {:?}",
                   data_id,
                   error);
            routing_node
                .send_put_mdata_response(dst, src, Err(error), msg_id)?;
            true
        } else {
            false
        };

        self.update_pending_writes(routing_node, mutation, src, dst, msg_id, rejected)
    }

    pub fn handle_get_mdata_shell(&mut self,
                                  routing_node: &mut RoutingNode,
                                  src: Authority<XorName>,
                                  dst: Authority<XorName>,
                                  name: XorName,
                                  tag: u64,
                                  msg_id: MessageId)
                                  -> Result<(), InternalError> {
        self.update_request_stats(&src);
        let res = self.fetch_mdata(name, tag).map(|data| data.shell());
        routing_node
            .send_get_mdata_shell_response(dst, src, res, msg_id)?;
        Ok(())
    }

    pub fn handle_get_mdata_shell_success(&mut self,
                                          routing_node: &mut RoutingNode,
                                          src: XorName,
                                          mut shell: MutableData)
                                          -> Result<(), InternalError> {
        let valid = if let Some(fragment) = self.cache.stop_needed_fragment_request(&src) {
            let actual_hash = utils::mdata_shell_hash(&shell);

            match fragment {
                FragmentInfo::MutableDataShell { hash, .. } if hash == actual_hash => {
                    self.cache.remove_needed_fragment(&fragment);
                    true
                }
                _ => false,
            }
        } else {
            false
        };

        self.request_needed_data(routing_node)?;

        if !valid {
            return Ok(());
        }

        // If we're no longer in the close group, return.
        if !close_to_address(routing_node, shell.name()) {
            return Ok(());
        }

        let data_id = shell.id();
        let new = match self.chunk_store.get(&data_id) {
            Ok(ref old_data) if old_data.version() >= shell.version() => {
                // The data in the chunk store is already more recent than the
                // shell we received. Ignore it.
                return Ok(());
            }
            Ok(ref old_data) => {
                // The shell is more recent than the data in the chunk store.
                // Repace the data with the shell, but keep the entries.
                for (key, value) in old_data.entries() {
                    shell.mutate_entry_without_validation(key.clone(), value.clone());
                }

                false
            }
            Err(_) => {
                // If we have cached entries for this data, apply them.
                for (key, value) in self.mdata_cache.take_entries(&shell.id()) {
                    // OK to ingore the return value here because as the shell has no
                    // entries, this call can never fail.
                    let _ = shell.mutate_entry_without_validation(key, value);
                }

                true
            }
        };

        self.clean_chunk_store();
        self.chunk_store.put(&data_id, &shell)?;

        if new {
            self.mutable_data_count += 1;
            log_status!(self);
        }

        Ok(())
    }

    pub fn handle_get_mdata_shell_failure(&mut self,
                                          routing_node: &mut RoutingNode,
                                          src: XorName)
                                          -> Result<(), InternalError> {
        if self.cache.stop_needed_fragment_request(&src).is_none() {
            return Err(InternalError::InvalidMessage);
        }

        self.request_needed_data(routing_node)
    }

    pub fn handle_get_mdata_version(&mut self,
                                    routing_node: &mut RoutingNode,
                                    src: Authority<XorName>,
                                    dst: Authority<XorName>,
                                    name: XorName,
                                    tag: u64,
                                    msg_id: MessageId)
                                    -> Result<(), InternalError> {
        self.update_request_stats(&src);
        let res = self.fetch_mdata(name, tag).map(|data| data.version());
        routing_node
            .send_get_mdata_version_response(dst, src, res, msg_id)?;
        Ok(())
    }

    pub fn handle_list_mdata_entries(&mut self,
                                     routing_node: &mut RoutingNode,
                                     src: Authority<XorName>,
                                     dst: Authority<XorName>,
                                     name: XorName,
                                     tag: u64,
                                     msg_id: MessageId)
                                     -> Result<(), InternalError> {
        self.update_request_stats(&src);
        let res = self.fetch_mdata(name, tag)
            .map(|data| data.entries().clone());
        routing_node
            .send_list_mdata_entries_response(dst, src, res, msg_id)?;
        Ok(())
    }

    pub fn handle_list_mdata_keys(&mut self,
                                  routing_node: &mut RoutingNode,
                                  src: Authority<XorName>,
                                  dst: Authority<XorName>,
                                  name: XorName,
                                  tag: u64,
                                  msg_id: MessageId)
                                  -> Result<(), InternalError> {
        self.update_request_stats(&src);
        let res = self.fetch_mdata(name, tag)
            .map(|data| data.entries().keys().cloned().collect());
        routing_node
            .send_list_mdata_keys_response(dst, src, res, msg_id)?;
        Ok(())
    }

    pub fn handle_list_mdata_values(&mut self,
                                    routing_node: &mut RoutingNode,
                                    src: Authority<XorName>,
                                    dst: Authority<XorName>,
                                    name: XorName,
                                    tag: u64,
                                    msg_id: MessageId)
                                    -> Result<(), InternalError> {
        self.update_request_stats(&src);
        let res = self.fetch_mdata(name, tag)
            .map(|data| data.entries().values().cloned().collect());
        routing_node
            .send_list_mdata_values_response(dst, src, res, msg_id)?;
        Ok(())
    }

    #[cfg_attr(feature = "cargo-clippy", allow(too_many_arguments))]
    pub fn handle_get_mdata_value(&mut self,
                                  routing_node: &mut RoutingNode,
                                  src: Authority<XorName>,
                                  dst: Authority<XorName>,
                                  name: XorName,
                                  tag: u64,
                                  key: Vec<u8>,
                                  msg_id: MessageId)
                                  -> Result<(), InternalError> {
        self.update_request_stats(&src);
        let res = self.fetch_mdata(name, tag)
            .and_then(|data| data.get(&key).cloned().ok_or(ClientError::NoSuchEntry));
        routing_node
            .send_get_mdata_value_response(dst, src, res, msg_id)?;
        Ok(())
    }

    pub fn handle_get_mdata_value_success(&mut self,
                                          routing_node: &mut RoutingNode,
                                          src: XorName,
                                          value: Value)
                                          -> Result<(), InternalError> {
        let info = if let Some(fragment) = self.cache.stop_needed_fragment_request(&src) {
            let actual_hash = utils::mdata_value_hash(&value);

            match fragment {
                FragmentInfo::MutableDataEntry {
                    name,
                    tag,
                    ref key,
                    hash,
                    ..
                } if hash == actual_hash => {
                    self.cache.remove_needed_fragment(&fragment);
                    Some((name, tag, key.clone()))
                }
                _ => None,
            }
        } else {
            None
        };

        self.request_needed_data(routing_node)?;

        let (name, tag, key) = if let Some(info) = info {
            info
        } else {
            return Ok(());
        };

        // If we're no longer in the close group, return.
        if !close_to_address(routing_node, &name) {
            return Ok(());
        }

        let data_id = MutableDataId(name, tag);
        match self.chunk_store.get(&data_id) {
            Ok(mut data) => {
                if data.mutate_entry_without_validation(key, value) {
                    self.clean_chunk_store();
                    self.chunk_store.put(&data_id, &data)?;
                }
            }
            Err(_) => {
                // We don't have the shell yet, so keep the entry around in the cache
                // until we receive the shell.
                self.mdata_cache.insert_entry(data_id, key, value);
            }
        }

        Ok(())
    }

    pub fn handle_get_mdata_value_failure(&mut self,
                                          routing_node: &mut RoutingNode,
                                          src: XorName)
                                          -> Result<(), InternalError> {
        if self.cache.stop_needed_fragment_request(&src).is_none() {
            return Err(InternalError::InvalidMessage);
        }

        self.request_needed_data(routing_node)
    }

    #[cfg_attr(feature = "cargo-clippy", allow(too_many_arguments))]
    pub fn handle_mutate_mdata_entries(&mut self,
                                       routing_node: &mut RoutingNode,
                                       src: Authority<XorName>,
                                       dst: Authority<XorName>,
                                       name: XorName,
                                       tag: u64,
                                       actions: BTreeMap<Vec<u8>, EntryAction>,
                                       msg_id: MessageId,
                                       requester: sign::PublicKey)
                                       -> Result<(), InternalError> {
        let mutation = Mutation::MutateMDataEntries {
            name: name,
            tag: tag,
            actions: actions.clone(),
        };
        let res = self.fetch_mdata(name, tag)
            .and_then(|data| {
                          data.clone().mutate_entries(actions.clone(), requester)?;
                          self.validate_concurrent_mutations(Some(&data), &mutation)
                      });

        self.start_pending_mutation(routing_node, src, dst, mutation, res, msg_id)
    }

    pub fn handle_list_mdata_permissions(&mut self,
                                         routing_node: &mut RoutingNode,
                                         src: Authority<XorName>,
                                         dst: Authority<XorName>,
                                         name: XorName,
                                         tag: u64,
                                         msg_id: MessageId)
                                         -> Result<(), InternalError> {
        self.update_request_stats(&src);
        let res = self.fetch_mdata(name, tag)
            .map(|data| data.permissions().clone());
        routing_node
            .send_list_mdata_permissions_response(dst, src, res, msg_id)?;
        Ok(())
    }

    #[cfg_attr(feature = "cargo-clippy", allow(too_many_arguments))]
    pub fn handle_list_mdata_user_permissions(&mut self,
                                              routing_node: &mut RoutingNode,
                                              src: Authority<XorName>,
                                              dst: Authority<XorName>,
                                              name: XorName,
                                              tag: u64,
                                              user: User,
                                              msg_id: MessageId)
                                              -> Result<(), InternalError> {
        self.update_request_stats(&src);
        let res = self.fetch_mdata(name, tag)
            .and_then(|data| data.user_permissions(&user).map(|p| *p));
        routing_node
            .send_list_mdata_user_permissions_response(dst, src, res, msg_id)?;
        Ok(())
    }

    #[cfg_attr(feature = "cargo-clippy", allow(too_many_arguments))]
    pub fn handle_set_mdata_user_permissions(&mut self,
                                             routing_node: &mut RoutingNode,
                                             src: Authority<XorName>,
                                             dst: Authority<XorName>,
                                             name: XorName,
                                             tag: u64,
                                             user: User,
                                             permissions: PermissionSet,
                                             version: u64,
                                             msg_id: MessageId,
                                             requester: sign::PublicKey)
                                             -> Result<(), InternalError> {
        let mutation = Mutation::SetMDataUserPermissions {
            name: name,
            tag: tag,
            user: user,
            permissions: permissions,
            version: version,
        };
        let res = self.fetch_mdata(name, tag)
            .and_then(|data| {
                          data.clone()
                              .set_user_permissions(user, permissions, version, requester)?;
                          self.validate_concurrent_mutations(Some(&data), &mutation)
                      });
        self.start_pending_mutation(routing_node, src, dst, mutation, res, msg_id)
    }

    #[cfg_attr(feature = "cargo-clippy", allow(too_many_arguments))]
    pub fn handle_del_mdata_user_permissions(&mut self,
                                             routing_node: &mut RoutingNode,
                                             src: Authority<XorName>,
                                             dst: Authority<XorName>,
                                             name: XorName,
                                             tag: u64,
                                             user: User,
                                             version: u64,
                                             msg_id: MessageId,
                                             requester: sign::PublicKey)
                                             -> Result<(), InternalError> {
        let mutation = Mutation::DelMDataUserPermissions {
            name: name,
            tag: tag,
            user: user,
            version: version,
        };
        let res = self.fetch_mdata(name, tag)
            .and_then(|data| {
                          data.clone()
                              .del_user_permissions(&user, version, requester)?;
                          self.validate_concurrent_mutations(Some(&data), &mutation)
                      });
        self.start_pending_mutation(routing_node, src, dst, mutation, res, msg_id)
    }

    #[cfg_attr(feature = "cargo-clippy", allow(too_many_arguments))]
    pub fn handle_change_mdata_owner(&mut self,
                                     routing_node: &mut RoutingNode,
                                     src: ClientManagerAuthority,
                                     dst: Authority<XorName>,
                                     name: XorName,
                                     tag: u64,
                                     new_owners: BTreeSet<sign::PublicKey>,
                                     version: u64,
                                     msg_id: MessageId)
                                     -> Result<(), InternalError> {
        let mutation = Mutation::ChangeMDataOwner {
            name: name,
            tag: tag,
            new_owners: new_owners.clone(),
            version: version,
        };

        let res = self.fetch_mdata(name, tag)
            .and_then(|data| {
                let new_owner = extract_owner(new_owners)?;

                if utils::verify_mdata_owner(&data, src.name()) {
                    data.clone().change_owner(new_owner, version)?;
                    self.validate_concurrent_mutations(Some(&data), &mutation)
                } else {
                    Err(ClientError::AccessDenied)
                }
            });

        self.start_pending_mutation(routing_node, src.into(), dst, mutation, res, msg_id)
    }

    pub fn handle_node_added(&mut self,
                             routing_node: &mut RoutingNode,
                             node_name: &XorName,
                             routing_table: &RoutingTable<XorName>)
                             -> Result<(), InternalError> {
        if self.cache.prune_needed_fragments(routing_table) {
            let _ = self.request_needed_data(routing_node);
        }

        let mut refreshes = Vec::new();
        let mut has_pruned_data = false;

        for data_id in self.our_chunks() {
            // Only retain chunks for which we're still in the close group.
            match routing_table.other_closest_names(data_id.name(), GROUP_SIZE) {
                None => {
                    trace!("No longer a DM for {:?}", data_id);

                    match data_id {
                        DataId::Immutable(idata_id) => {
                            if self.chunk_store.has(&idata_id) &&
                               !self.cache.is_in_unneeded(&idata_id) {
                                self.immutable_data_count -= 1;
                                has_pruned_data = true;
                                self.cache.add_as_unneeded(idata_id);
                            }
                        }
                        DataId::Mutable(mdata_id) => {
                            if self.chunk_store.has(&mdata_id) {
                                self.mutable_data_count -= 1;
                                has_pruned_data = true;
                                let _ = self.chunk_store.delete(&mdata_id);
                            }
                        }
                    }
                }
                Some(close_group) => {
                    if close_group.contains(&node_name) {
                        refreshes.push(Refresh::from_data_id(data_id));
                    }
                }
            }
        }

        if !refreshes.is_empty() {
            let _ = self.send_refresh(routing_node, Authority::ManagedNode(*node_name), refreshes);
        }

        if has_pruned_data {
            log_status!(self);
        }

        Ok(())
    }

    /// Get all names and hashes of all data. // [TODO]: Can be optimised - 2016-04-23 09:11pm
    /// Send to all members of group of data.
    pub fn handle_node_lost(&mut self,
                            routing_node: &mut RoutingNode,
                            node_name: &XorName,
                            routing_table: &RoutingTable<XorName>)
                            -> Result<(), InternalError> {
        let pruned_unneeded_chunks = self.cache.prune_unneeded_chunks(routing_table);
        if pruned_unneeded_chunks != 0 {
            self.immutable_data_count += pruned_unneeded_chunks;
            log_status!(self);
        }

        if self.cache.prune_needed_fragments(routing_table) {
            self.request_needed_data(routing_node)?;
        }

        let mut refreshes = HashMap::default();
        for data_id in self.our_chunks() {
            match routing_table.other_closest_names(data_id.name(), GROUP_SIZE) {
                None => {
                    error!("Moved out of close group of {:?} in a NodeLost event.",
                           node_name);
                }
                Some(close_group) => {
                    // If no new node joined the group due to this event, continue:
                    // If the group has fewer than GROUP_SIZE elements, the lost node was not
                    // replaced at all. Otherwise, if the group's last node is closer to the data
                    // than the lost node, the lost node was not in the group in the first place.
                    if let Some(&outer_node) = close_group.get(GROUP_SIZE - 2) {
                        if data_id.name().closer(node_name, outer_node) {
                            refreshes
                                .entry(*outer_node)
                                .or_insert_with(Vec::new)
                                .push(Refresh::from_data_id(data_id))
                        }
                    }
                }
            }
        }

        for (node_name, refresh) in refreshes {
            let _ = self.send_refresh(routing_node, Authority::ManagedNode(node_name), refresh);
        }

        Ok(())
    }

    pub fn check_timeouts(&mut self, routing_node: &mut RoutingNode) {
        let _ = self.request_needed_data(routing_node);
    }

    fn fetch_mdata(&self, name: XorName, tag: u64) -> Result<MutableData, ClientError> {
        let data_id = MutableDataId(name, tag);
        if let Ok(data) = self.chunk_store.get(&data_id) {
            Ok(data)
        } else if tag == TYPE_TAG_SESSION_PACKET {
            Err(ClientError::NoSuchAccount)
        } else {
            Err(ClientError::NoSuchData)
        }
    }

    fn update_request_stats(&mut self, src: &Authority<XorName>) {
        if let Authority::Client { .. } = *src {
            self.client_get_requests += 1;
            log_status!(self);
        }
    }

    fn update_pending_writes(&mut self,
                             routing_node: &mut RoutingNode,
                             mutation: Mutation,
                             src: Authority<XorName>,
                             dst: Authority<XorName>,
                             message_id: MessageId,
                             rejected: bool)
                             -> Result<(), InternalError> {
        for PendingWrite {
                mutation,
                src,
                dst,
                message_id,
                ..
            } in self.cache.remove_expired_pending_writes() {
            let error = ClientError::from("Request expired.");
            trace!("{:?} did not accumulate. Sending failure",
                   mutation.data_id());
            self.send_mutation_response(routing_node,
                                        src,
                                        dst,
                                        mutation.mutation_type(),
                                        mutation.data_id(),
                                        Err(error),
                                        message_id)?;
        }

        let data_id = mutation.data_id();
        if let Some(refresh) = self.cache
               .insert_pending_write(mutation, src, dst, message_id, rejected) {
            self.send_group_refresh(routing_node, *data_id.name(), refresh, message_id)?;
        }

        Ok(())
    }

    fn start_pending_mutation(&mut self,
                              routing_node: &mut RoutingNode,
                              src: Authority<XorName>,
                              dst: Authority<XorName>,
                              mutation: Mutation,
                              res: Result<(), ClientError>,
                              msg_id: MessageId)
                              -> Result<(), InternalError> {
        let mutation_type = mutation.mutation_type();
        let data_id = mutation.data_id();

        self.update_pending_writes(routing_node, mutation, src, dst, msg_id, res.is_err())?;

        if let Err(error) = res {
            self.send_mutation_response(routing_node,
                                        src,
                                        dst,
                                        mutation_type,
                                        data_id,
                                        Err(error),
                                        msg_id)
        } else {
            Ok(())
        }
    }

    fn commit_pending_mutation(&mut self,
                               routing_node: &mut RoutingNode,
                               src: Authority<XorName>,
                               dst: Authority<XorName>,
                               mutation: Mutation,
                               msg_id: MessageId)
                               -> Result<Vec<FragmentInfo>, InternalError> {
        let mutation_type = mutation.mutation_type();
        let data_id = mutation.data_id();

        let res = match mutation {
            Mutation::PutIData(data) => {
                let fragments = vec![FragmentInfo::ImmutableData(*data.name())];
                put_into_chunk_store(&mut self.chunk_store, &data).map(|_| fragments)
            }
            Mutation::PutMData(data) => {
                let fragments = FragmentInfo::mutable_data(&data);
                put_into_chunk_store(&mut self.chunk_store, &data).map(|_| fragments)
            }
            Mutation::MutateMDataEntries { name, tag, actions } => {
                self.with_mdata(name, tag, |data| {
                    let keys: Vec<_> = actions.keys().cloned().collect();
                    data.mutate_entries_without_validation(actions);
                    keys.into_iter()
                        .filter_map(|key| {
                                        data.get(&key)
                                            .map(|value| {
                                                     FragmentInfo::mutable_data_entry(data,
                                                                                      key,
                                                                                      value)
                                                 })
                                    })
                        .collect()
                })
            }
            Mutation::SetMDataUserPermissions {
                name,
                tag,
                user,
                permissions,
                version,
            } => {
                self.with_mdata(name, tag, |data| {
                    data.set_user_permissions_without_validation(user, permissions, version);
                    vec![FragmentInfo::mutable_data_shell(data)]
                })
            }
            Mutation::DelMDataUserPermissions {
                name,
                tag,
                user,
                version,
            } => {
                self.with_mdata(name, tag, |data| {
                    data.del_user_permissions_without_validation(&user, version);
                    vec![FragmentInfo::mutable_data_shell(data)]
                })
            }
            Mutation::ChangeMDataOwner {
                name,
                tag,
                new_owners,
                version,
            } => {
                self.with_mdata(name, tag, |data| {
                    if let Some(new_owner) = new_owners.into_iter().next() {
                        data.change_owner_without_validation(new_owner, version);
                    }
                    vec![FragmentInfo::mutable_data_shell(data)]
                })
            }
        };

        let (res, fragments) = match res {
            Ok(fragments) => (Ok(()), fragments),
            Err(error) => (Err(error), Vec::new()),
        };

        self.send_mutation_response(routing_node, src, dst, mutation_type, data_id, res, msg_id)?;
        Ok(fragments)
    }

    #[cfg_attr(feature = "cargo-clippy", allow(too_many_arguments))]
    fn send_mutation_response(&self,
                              routing_node: &mut RoutingNode,
                              src: Authority<XorName>,
                              dst: Authority<XorName>,
                              mutation_type: MutationType,
                              data_id: DataId,
                              res: Result<(), ClientError>,
                              msg_id: MessageId)
                              -> Result<(), InternalError> {
        let res_string = match res {
            Ok(_) => "success".to_string(),
            Err(ref error) => format!("failure ({:?})", error),
        };

        match mutation_type {
            MutationType::PutIData => {
                trace!("DM sending PutIData {} for data {:?}", res_string, data_id);
                routing_node
                    .send_put_idata_response(dst, src, res, msg_id)?
            }
            MutationType::PutMData => {
                trace!("DM sending PutMData {} for data {:?}", res_string, data_id);
                routing_node
                    .send_put_mdata_response(dst, src, res, msg_id)?
            }
            MutationType::MutateMDataEntries => {
                trace!("DM sending MutateMDataEntries {} for data {:?}",
                       res_string,
                       data_id);
                routing_node
                    .send_mutate_mdata_entries_response(dst, src, res, msg_id)?
            }
            MutationType::SetMDataUserPermissions => {
                trace!("DM sending SetMDataUserPermissions {} for data {:?}",
                       res_string,
                       data_id);
                routing_node
                    .send_set_mdata_user_permissions_response(dst, src, res, msg_id)?
            }
            MutationType::DelMDataUserPermissions => {
                trace!("DM sending DelMDataUserPermissions {} for data {:?}",
                       res_string,
                       data_id);
                routing_node
                    .send_del_mdata_user_permissions_response(dst, src, res, msg_id)?
            }
            MutationType::ChangeMDataOwner => {
                trace!("DM sending ChangeMDataOwner {} for data {:?}",
                       res_string,
                       data_id);
                routing_node
                    .send_change_mdata_owner_response(dst, src, res, msg_id)?
            }
        }

        Ok(())
    }

    /// Returns whether our data uses more than `MAX_FULL_PERCENT` percent of available space.
    fn chunk_store_full(&self) -> bool {
        self.chunk_store.used_space() > (self.chunk_store.max_space() / 100) * MAX_FULL_PERCENT
    }

    /// Removes data chunks we are no longer responsible for until the chunk store is not full
    /// anymore.
    fn clean_chunk_store(&mut self) {
        while self.chunk_store_full() {
            if let Some(data_id) = self.cache.pop_unneeded_chunk() {
                if let Err(error) = self.chunk_store.delete(&data_id) {
                    warn!("DM failed to delete unneeded chunk {:?}: {:?}",
                          data_id,
                          error);
                    break;
                }
            } else {
                break;
            }
        }
    }

    fn handle_fragment_refresh(&mut self,
                               src: XorName,
                               fragment: FragmentInfo)
                               -> Result<(), InternalError> {
        if self.cache
               .register_needed_fragment_with_another_holder(fragment.clone(), src) {
            return Ok(());
        }

        let holders = match self.fragment_refresh_accumulator
                  .add(fragment.clone(), src)
                  .cloned() {
            Some(holders) => {
                self.fragment_refresh_accumulator.delete(&fragment);
                holders
            }
            None => return Ok(()),
        };

        let needed = match fragment {
            FragmentInfo::ImmutableData(name) => !self.chunk_store.has(&ImmutableDataId(name)),
            FragmentInfo::MutableDataShell { name, tag, version, .. } => {
                match self.chunk_store.get(&MutableDataId(name, tag)) {
                    Err(_) => true,
                    Ok(data) => data.version() < version,
                }
            }
            FragmentInfo::MutableDataEntry {
                name,
                tag,
                ref key,
                version,
                ..
            } => {
                if let Ok(data) = self.chunk_store.get(&MutableDataId(name, tag)) {
                    match data.get(key) {
                        None => true,
                        Some(value) => value.entry_version < version,
                    }
                } else {
                    true
                }
            }
        };

        if needed {
            for holder in holders {
                self.cache
                    .insert_needed_fragment(fragment.clone(), holder);
            }
        }

        Ok(())
    }

    fn handle_chunk_refresh(&mut self,
                            src: XorName,
                            data_id: MutableDataId)
                            -> Result<(), InternalError> {
        if self.chunk_refresh_accumulator
               .add(data_id, src)
               .is_some() {
            self.chunk_refresh_accumulator.delete(&data_id);
            self.cache.insert_needed_mutable_chunk(data_id);
        }

        Ok(())
    }

    fn send_refresh(&self,
                    routing_node: &mut RoutingNode,
                    dst: Authority<XorName>,
                    refreshes: Vec<Refresh>)
                    -> Result<(), InternalError> {
        // FIXME - We need to handle >2MB chunks
        let src = Authority::ManagedNode(*routing_node.id()?.name());
        let payload = if dst.is_single() {
            serialisation::serialise(&VaultRefresh::DataManager(refreshes))?
        } else {
            serialisation::serialise(&refreshes)?
        };
        trace!("DM sending refresh to {:?}.", dst);
        routing_node
            .send_refresh_request(src, dst, payload, MessageId::new())?;
        Ok(())
    }

    fn send_group_refresh(&self,
                          routing_node: &mut RoutingNode,
                          name: XorName,
                          refresh: MutationVote,
                          msg_id: MessageId)
                          -> Result<(), InternalError> {
        let payload = serialisation::serialise(&refresh)?;
        trace!("DM sending refresh data to group {:?}.", name);
        routing_node
            .send_refresh_request(Authority::NaeManager(name),
                                  Authority::NaeManager(name),
                                  payload,
                                  msg_id)?;
        Ok(())
    }

    fn request_needed_data(&mut self, routing_node: &mut RoutingNode) -> Result<(), InternalError> {
        self.request_needed_chunks(routing_node)?;
        self.request_needed_fragments(routing_node)?;
        Ok(())
    }

    fn request_needed_chunks(&mut self,
                             routing_node: &mut RoutingNode)
                             -> Result<(), InternalError> {
        if let Some(data_id) = self.cache.needed_mutable_chunk() {
            let MutableDataId(name, tag) = data_id;
            let src = Authority::ManagedNode(*routing_node.id()?.name());
            let dst = Authority::NaeManager(name);
            let msg_id = MessageId::new();

            routing_node
                .send_get_mdata_request(src, dst, name, tag, msg_id)?;
            self.cache
                .start_needed_mutable_chunk_request(data_id, msg_id);
        }

        Ok(())
    }

    fn request_needed_fragments(&mut self,
                                routing_node: &mut RoutingNode)
                                -> Result<(), InternalError> {
        let src = Authority::ManagedNode(*routing_node.id()?.name());
        let candidates = self.cache.needed_fragments();

        // Set of holders we already sent a request to. Used to prevent sending
        // multiple request to a single holder.
        let mut busy_holders = HashSet::default();

        // For each fragment type except `MutableData`, send at most one request.
        // For `MutableData`, send a request to each member of its close group.
        for (fragment, holders) in candidates {
            for holder in holders {
                if !is_in_close_group(routing_node, fragment.name(), &holder) {
                    continue;
                }

                if !busy_holders.insert(holder) {
                    continue;
                }

                self.cache
                    .start_needed_fragment_request(&fragment, &holder);

                let dst = Authority::ManagedNode(holder);
                let msg_id = MessageId::new();

                match fragment {
                    FragmentInfo::ImmutableData(name) => {
                        routing_node
                            .send_get_idata_request(src, dst, name, msg_id)?;
                        break;
                    }
                    FragmentInfo::MutableDataShell { name, tag, .. } => {
                        routing_node
                            .send_get_mdata_shell_request(src, dst, name, tag, msg_id)?;
                        break;
                    }
                    FragmentInfo::MutableDataEntry { name, tag, key, .. } => {
                        routing_node
                            .send_get_mdata_value_request(src, dst, name, tag, key, msg_id)?;
                        break;
                    }
                }
            }
        }

        self.cache.print_stats();
        Ok(())
    }

    // Get IDs of all the data chunks we are responsible for, regardless of whether
    // we already have them or not.
    fn our_chunks(&self) -> HashSet<DataId> {
        let mut result = self.cache.needed_chunks();
        result.extend(self.chunk_store.keys());
        result
    }

    // Load mutable data from the chunk store, apply the given function to it put it back.
    fn with_mdata<F, R>(&mut self, name: XorName, tag: u64, f: F) -> Result<R, ClientError>
        where F: FnOnce(&mut MutableData) -> R
    {
        let mut data = get_from_chunk_store(&self.chunk_store, &MutableDataId(name, tag))?;
        let result = f(&mut data);
        put_into_chunk_store(&mut self.chunk_store, &data)?;
        Ok(result)
    }

    fn validate_concurrent_mutations(&self,
                                     existing_data: Option<&MutableData>,
                                     new_mutation: &Mutation)
                                     -> Result<(), ClientError> {
        if self.cache
               .validate_concurrent_mutations(existing_data, new_mutation) {
            Ok(())
        } else {
            // TODO: consider adding `Conflict` variant to `ClientError`.
            Err(ClientError::from("Conflicting concurrent mutation"))
        }
    }
}

#[cfg(feature = "use-mock-crust")]
impl DataManager {
    pub fn get_stored_ids_and_versions(&self) -> Result<Vec<(DataId, u64)>, ChunkStoreError> {
        let data_ids = self.chunk_store.keys();
        let mut result = Vec::with_capacity(data_ids.len());

        for data_id in data_ids {
            if let DataId::Immutable(ref id) = data_id {
                if self.cache.is_in_unneeded(id) {
                    continue;
                }
            }

            let version = self.get_version(&data_id)?;
            result.push((data_id, version));
        }

        Ok(result)
    }

    pub fn delay_group_refreshes(&mut self, delayed: bool) {
        self._is_delaying_group_refresh = delayed;
    }

    // If `serialised_refresh` has not been previously handled here, it is added to the cache for
    // later and the function returns `None`.  If it has been seen before, it is popped from the
    // cache and returned.
    fn handle_delayed_group_refresh(&mut self, serialised_refresh: Vec<u8>) -> Option<Vec<u8>> {
        if !self._delayed_group_refresh_cache
                .remove(&serialised_refresh) && self._is_delaying_group_refresh {
            let _ = self._delayed_group_refresh_cache
                .insert(serialised_refresh);
            return None;
        }
        Some(serialised_refresh)
    }

    // In real network, the group_refresh is sent in the highest priority and shall not be dropped.
    // However, it may get severely delayed for particular node, hence being processed out-of-order.
    pub fn pop_group_refresh(&mut self, routing_node: &mut RoutingNode) {
        use itertools::Itertools;
        use maidsafe_utilities::SeededRng;
        use rand::Rng;

        let mut rng = SeededRng::thread_rng();
        if rng.gen_weighted_bool(10) {
            let _ =
                rng.choose(&self._delayed_group_refresh_cache.iter().collect_vec())
                    .cloned()
                    .cloned()
                    .map(|delayed_refresh| {
                             self.handle_group_refresh(routing_node, delayed_refresh)
                         });
        }
    }

    fn get_version(&self, data_id: &DataId) -> Result<u64, ChunkStoreError> {
        match *data_id {
            DataId::Immutable(_) => Ok(0),
            DataId::Mutable(ref data_id) => {
                self.chunk_store.get(data_id).map(|data| data.version())
            }
        }
    }
}

#[cfg(all(test, feature = "use-mock-routing"))]
impl DataManager {
    /// For testing only - put the given data directly into the chunks store.
    pub fn put_into_chunk_store<T, I>(&mut self, data: T)
        where T: Chunk<DataId, Id = I> + Data<Id = I>,
              I: ChunkId<DataId>
    {
        unwrap!(self.chunk_store.put(&data.id(), &data))
    }

    /// For testing only - retrieve data with the given ID from the chunk store.
    pub fn get_from_chunk_store<T: ChunkId<DataId>>(&self, data_id: &T) -> Option<T::Chunk> {
        self.chunk_store.get(data_id).ok()
    }

    /// Clear the cache
    pub fn clear_cache(&mut self) {
        self.cache.clear();
        self.mdata_cache.clear();
    }
}

impl Debug for DataManager {
    fn fmt(&self, formatter: &mut Formatter) -> fmt::Result {
        write!(formatter,
               "This vault has received {} Client Get requests. Chunks stored: Immutable: {}, \
                Mutable: {}. Total stored: {} bytes.",
               self.client_get_requests,
               self.immutable_data_count,
               self.mutable_data_count,
               self.chunk_store.used_space())
    }
}

#[derive(Deserialize, Serialize, PartialEq, Eq, Debug, Clone)]
pub enum Refresh {
    Chunk(MutableDataId),
    Fragment(FragmentInfo),
}

impl Refresh {
    fn from_data_id(id: DataId) -> Self {
        match id {
            DataId::Immutable(ImmutableDataId(name)) => {
                Refresh::Fragment(FragmentInfo::ImmutableData(name))
            }
            DataId::Mutable(id) => Refresh::Chunk(id),
        }
    }
}

fn close_to_address(routing_node: &mut RoutingNode, address: &XorName) -> bool {
    routing_node.close_group(*address, GROUP_SIZE).is_some()
}

// Is `name` in the close group of `group_name`?
fn is_in_close_group(routing_node: &mut RoutingNode, group_name: &XorName, name: &XorName) -> bool {
    routing_node
        .close_group(*group_name, GROUP_SIZE)
        .map_or(false, |group| group.contains(name))
}

fn recompute_idata_name(data: &ImmutableData) -> XorName {
    XorName(tiny_keccak::sha3_256(data.value()))
}

/// Merges the entries that are currently in the data with the new entries.
/// Entries that are not present in the data are inserted. Those that are present
/// are overwritten if the new version is higher than the old version, otherwise
/// ignored.
fn merge_mdata_entries<I>(data: &mut MutableData, entries: I)
    where I: IntoIterator<Item = (Vec<u8>, Value)>
{
    for (key, value) in entries {
        data.mutate_entry_without_validation(key, value);
    }
}

// `owners` must have exactly 1 element.
fn extract_owner(owners: BTreeSet<sign::PublicKey>) -> Result<sign::PublicKey, ClientError> {
    let len = owners.len();
    match owners.into_iter().next() {
        Some(owner) if len == 1 => Ok(owner),
        Some(_) | None => Err(ClientError::InvalidOwners),
    }
}

fn get_from_chunk_store<T: ChunkId<DataId> + Debug>(chunk_store: &ChunkStore<DataId>,
                                                    data_id: &T)
                                                    -> Result<T::Chunk, ClientError> {
    chunk_store
        .get(data_id)
        .map_err(|error| {
                     trace!("DM failed to load {:?} from chunkstore: {:?}",
                            data_id,
                            error);
                     ClientError::from(format!("Failed to load chunk: {:?}", error))
                 })
}

fn put_into_chunk_store<T, I>(chunk_store: &mut ChunkStore<DataId>,
                              data: &T)
                              -> Result<(), ClientError>
    where T: Chunk<DataId, Id = I> + Data<Id = I>,
          I: ChunkId<DataId> + Debug
{
    chunk_store
        .put(&data.id(), data)
        .map_err(|error| {
                     trace!("DM failed to store {:?} in chunkstore: {:?}",
                            data.id(),
                            error);
                     ClientError::from(format!("Failed to store chunk: {:?}", error))
                 })
}
